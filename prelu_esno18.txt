Namespace(activation='prelu', arch='resnet18', cuda='0', optimizer='SGD', root='D:/qpsk_awgn_sps8_esno18.dat')
ResNet(
  (prelu): PReLU(num_parameters=1)
  (conv1): Conv1d(2, 64, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace)
  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (prelu1): PReLU(num_parameters=1)
      (prelu2): PReLU(num_parameters=1)
    )
    (1): BasicBlock(
      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (prelu1): PReLU(num_parameters=1)
      (prelu2): PReLU(num_parameters=1)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (prelu1): PReLU(num_parameters=1)
      (prelu2): PReLU(num_parameters=1)
    )
    (1): BasicBlock(
      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (prelu1): PReLU(num_parameters=1)
      (prelu2): PReLU(num_parameters=1)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (prelu1): PReLU(num_parameters=1)
      (prelu2): PReLU(num_parameters=1)
    )
    (1): BasicBlock(
      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (prelu1): PReLU(num_parameters=1)
      (prelu2): PReLU(num_parameters=1)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (prelu1): PReLU(num_parameters=1)
      (prelu2): PReLU(num_parameters=1)
    )
    (1): BasicBlock(
      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (prelu1): PReLU(num_parameters=1)
      (prelu2): PReLU(num_parameters=1)
    )
  )
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (fc): Linear(in_features=512, out_features=2, bias=True)
)
loading data
  epoch    accuracy    train_loss    valid_loss    cp       dur
-------  ----------  ------------  ------------  ----  --------
      1      0.9918        0.1422        0.0233     +  150.9394
      2      0.9945        0.0180        0.0151     +  149.2682
      3      0.9955        0.0111        0.0124     +  149.5034
      4      0.9946        0.0078        0.0144        149.0190
      5      0.9954        0.0058        0.0130        148.9149
      6      0.9960        0.0043        0.0111     +  149.1191
      7      0.9934        0.0033        0.0186        149.4443
      8      0.9957        0.0029        0.0115        149.3913
      9      0.9977        0.0021        0.0063     +  150.1678
     10      0.9977        0.0021        0.0063     +  149.1901
     11      0.9971        0.0017        0.0087        149.4233
     12      0.9978        0.0015        0.0061     +  148.6187
     13      0.9980        0.0010        0.0060     +  149.2011
     14      0.9980        0.0010        0.0059     +  148.7518
     15      0.9977        0.0010        0.0066        149.0610
     16      0.9981        0.0008        0.0064        149.7265
     17      0.9981        0.0007        0.0054     +  149.5114
     18      0.9981        0.0006        0.0057        148.0613
     19      0.9980        0.0006        0.0058        148.4286
     20      0.9980        0.0006        0.0056        148.7058
     21      0.9980        0.0005        0.0063        148.4085
     22      0.9983        0.0005        0.0051     +  149.4433
     23      0.9983        0.0004        0.0051     +  149.0290
     24      0.9983        0.0004        0.0051     +  149.2241
     25      0.9983        0.0003        0.0050     +  148.3665
     26      0.9981        0.0004        0.0054        148.7978
     27      0.9980        0.0004        0.0056        148.8228
     28      0.9974        0.0004        0.0083        147.8211
     29      0.9982        0.0003        0.0049     +  148.1824
     30      0.9980        0.0003        0.0060        148.5637
     31      0.9984        0.0002        0.0049     +  148.3255
     32      0.9982        0.0003        0.0054        148.7198
     33      0.9983        0.0003        0.0048     +  148.4155
     34      0.9983        0.0003        0.0049        148.6227
     35      0.9981        0.0002        0.0056        148.7498
     36      0.9978        0.0002        0.0070        148.7918
     37      0.9982        0.0002        0.0050        148.7598
Re-initializing optimizer because the following parameters were re-set: lr.
     38      0.9984        0.0002        0.0050        149.5994
     39      0.9983        0.0002        0.0051        149.2822
     40      0.9982        0.0002        0.0049        148.8068
     41      0.9983        0.0002        0.0049        148.3425
     42      0.9983        0.0002        0.0048     +  148.8579
     43      0.9983        0.0002        0.0053        148.6167
     44      0.9983        0.0002        0.0051        148.5266
     45      0.9983        0.0001        0.0048        148.3635
     46      0.9983        0.0002        0.0053        148.1894
     47      0.9983        0.0002        0.0048     +  148.5146
     48      0.9983        0.0001        0.0048        148.4035
     49      0.9982        0.0002        0.0048        147.9892
     50      0.9983        0.0001        0.0049        148.3015
     51      0.9983        0.0001        0.0048     +  148.2925
     52      0.9983        0.0002        0.0048        148.4135
     53      0.9984        0.0002        0.0050        148.3695
     54      0.9982        0.0001        0.0048        148.4045
     55      0.9983        0.0001        0.0049        147.3998
Re-initializing optimizer because the following parameters were re-set: lr.
     56      0.9983        0.0001        0.0048        142.0378
     57      0.9983        0.0001        0.0049        142.2830
     58      0.9983        0.0001        0.0048        142.2209
     59      0.9982        0.0001        0.0055        142.5051
     60      0.9984        0.0001        0.0050        142.4911
     61      0.9983        0.0001        0.0048        142.0788
     62      0.9983        0.0001        0.0048        141.9898
     63      0.9983        0.0002        0.0048        141.8987
     64      0.9983        0.0001        0.0048        141.8717
     65      0.9983        0.0001        0.0050        141.9447
     66      0.9983        0.0001        0.0048        142.0468
     67      0.9983        0.0001        0.0049        142.0468
     68      0.9983        0.0001        0.0049        141.9017
     69      0.9983        0.0001        0.0049        142.0618
     70      0.9984        0.0001        0.0049        142.0838
Stopping since valid_loss has not improved in the last 20 epochs.
Best Model State Restored
Best Confusion Matrix:
 [[100   0]
 [  0 100]]
