Namespace(activation='prelu', arch='resnet18', cuda='0', optimizer='SGD', root='D:/qpsk_awgn_sps8_esno16.dat')
ResNet(
  (prelu): PReLU(num_parameters=1)
  (conv1): Conv1d(2, 64, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace)
  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (prelu1): PReLU(num_parameters=1)
      (prelu2): PReLU(num_parameters=1)
    )
    (1): BasicBlock(
      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (prelu1): PReLU(num_parameters=1)
      (prelu2): PReLU(num_parameters=1)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (prelu1): PReLU(num_parameters=1)
      (prelu2): PReLU(num_parameters=1)
    )
    (1): BasicBlock(
      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (prelu1): PReLU(num_parameters=1)
      (prelu2): PReLU(num_parameters=1)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (prelu1): PReLU(num_parameters=1)
      (prelu2): PReLU(num_parameters=1)
    )
    (1): BasicBlock(
      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (prelu1): PReLU(num_parameters=1)
      (prelu2): PReLU(num_parameters=1)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (prelu1): PReLU(num_parameters=1)
      (prelu2): PReLU(num_parameters=1)
    )
    (1): BasicBlock(
      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (prelu1): PReLU(num_parameters=1)
      (prelu2): PReLU(num_parameters=1)
    )
  )
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (fc): Linear(in_features=512, out_features=2, bias=True)
)
loading data
  epoch    accuracy    train_loss    valid_loss    cp       dur
-------  ----------  ------------  ------------  ----  --------
      1      0.9797        0.2000        0.0550     +  149.0120
      2      0.9818        0.0440        0.0500     +  146.9965
      3      0.9873        0.0296        0.0336     +  146.6142
      4      0.9850        0.0220        0.0419        146.8774
      5      0.9665        0.0179        0.1004        146.3890
      6      0.9881        0.0151        0.0350        143.4689
      7      0.9862        0.0119        0.0372        140.0683
      8      0.9919        0.0107        0.0219     +  140.3195
      9      0.9826        0.0090        0.0539        140.1494
     10      0.9906        0.0076        0.0263        141.0160
     11      0.9877        0.0064        0.0360        140.5347
     12      0.9920        0.0057        0.0238        140.3906
Re-initializing optimizer because the following parameters were re-set: lr.
     13      0.9902        0.0051        0.0296        140.4526
     14      0.9929        0.0028        0.0211     +  140.4406
     15      0.9929        0.0024        0.0208     +  140.2575
     16      0.9928        0.0024        0.0208     +  140.4956
     17      0.9928        0.0023        0.0208     +  140.0633
     18      0.9927        0.0020        0.0215        140.3065
     19      0.9927        0.0020        0.0215        140.4696
     20      0.9929        0.0020        0.0207     +  140.1264
     21      0.9929        0.0019        0.0209        140.1134
     22      0.9928        0.0017        0.0210        140.2004
     23      0.9930        0.0018        0.0209        140.2044
     24      0.9929        0.0019        0.0211        139.7761
Re-initializing optimizer because the following parameters were re-set: lr.
     25      0.9928        0.0017        0.0209        139.9252
     26      0.9930        0.0017        0.0207     +  139.7811
     27      0.9930        0.0018        0.0209        139.6860
     28      0.9928        0.0016        0.0212        139.9272
     29      0.9930        0.0017        0.0210        139.8982
     30      0.9929        0.0017        0.0210        139.5449
Re-initializing optimizer because the following parameters were re-set: lr.
     31      0.9929        0.0017        0.0209        139.7611
     32      0.9930        0.0016        0.0207        140.2895
     33      0.9930        0.0015        0.0208        140.4576
     34      0.9929        0.0017        0.0210        139.8502
     35      0.9930        0.0018        0.0207        140.0083
     36      0.9930        0.0016        0.0209        139.7601
     37      0.9930        0.0017        0.0208        139.8712
     38      0.9931        0.0017        0.0208        140.1054
     39      0.9929        0.0017        0.0209        140.2735
     40      0.9929        0.0017        0.0214        140.1824
     41      0.9927        0.0016        0.0213        140.7218
     42      0.9931        0.0018        0.0207        141.2522
     43      0.9928        0.0016        0.0211        141.3723
     44      0.9929        0.0017        0.0211        141.0140
     45      0.9930        0.0016        0.0208        141.6625
Stopping since valid_loss has not improved in the last 20 epochs.
Best Model State Restored
Best Confusion Matrix:
 [[99  1]
 [ 1 99]]
