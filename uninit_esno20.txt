Namespace(activation='uninit_param', arch='resnet18', cuda='0', optimizer='SGD', root='D:/qpsk_awgn_sps8_esno20.dat')
ResNet(
  (conv1): Conv1d(2, 64, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace)
  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (fc): Linear(in_features=512, out_features=2, bias=True)
)
loading data
  epoch    accuracy    train_loss    valid_loss    cp       dur
-------  ----------  ------------  ------------  ----  --------
      1      0.9924        0.1675        0.0230     +  127.8943
      2      0.9961        0.0085        0.0119     +  126.1590
      3      0.9989        0.0042        0.0049     +  126.5052
      4      0.9988        0.0026        0.0038     +  126.6473
      5      0.9990        0.0019        0.0032     +  126.5232
      6      0.9989        0.0014        0.0035        126.5262
      7      0.9992        0.0011        0.0026     +  126.7074
      8      0.9992        0.0009        0.0024     +  126.6253
      9      0.9993        0.0008        0.0023     +  126.2811
     10      0.9992        0.0007        0.0023        125.9398
     11      0.9993        0.0005        0.0021     +  126.2500
     12      0.9989        0.0005        0.0029        125.9718
     13      0.9993        0.0005        0.0021        126.3401
     14      0.9993        0.0004        0.0020     +  126.3221
     15      0.9993        0.0004        0.0019     +  126.3041
     16      0.9993        0.0003        0.0019        126.2080
     17      0.9992        0.0003        0.0024        126.3291
     18      0.9994        0.0003        0.0018     +  126.1670
     19      0.9994        0.0003        0.0017     +  126.5763
     20      0.9994        0.0003        0.0018        126.3291
     21      0.9993        0.0002        0.0017        126.4071
     22      0.9995        0.0002        0.0017        126.1690
     23      0.9995        0.0002        0.0016     +  126.3051
     24      0.9993        0.0002        0.0016        126.1710
     25      0.9995        0.0002        0.0015     +  126.1440
     26      0.9994        0.0002        0.0016        126.2901
     27      0.9996        0.0002        0.0015        126.0169
     28      0.9995        0.0002        0.0015     +  123.5810
     29      0.9994        0.0002        0.0017        119.3869
     30      0.9995        0.0001        0.0015     +  119.5861
     31      0.9995        0.0002        0.0016        119.7702
     32      0.9995        0.0001        0.0016        119.7162
     33      0.9994        0.0001        0.0015        119.5230
     34      0.9995        0.0001        0.0015     +  119.6701
     35      0.9995        0.0001        0.0014     +  119.6121
     36      0.9994        0.0001        0.0014     +  119.5490
     37      0.9996        0.0001        0.0014     +  119.9473
     38      0.9994        0.0001        0.0015        119.3749
     39      0.9995        0.0001        0.0014     +  119.4990
     40      0.9995        0.0001        0.0014        119.4330
     41      0.9996        0.0001        0.0013     +  119.5220
     42      0.9996        0.0001        0.0013        120.3747
     43      0.9995        0.0001        0.0014        120.0664
     44      0.9994        0.0001        0.0014        120.1065
     45      0.9996        0.0001        0.0013        120.0484
     46      0.9996        0.0001        0.0013     +  120.0864
     47      0.9994        0.0001        0.0014        120.0494
     48      0.9996        0.0001        0.0013     +  120.3456
     49      0.9996        0.0001        0.0013        120.1165
     50      0.9996        0.0001        0.0013        119.9103
     51      0.9996        0.0001        0.0013        120.1245
     52      0.9995        0.0001        0.0013        119.9814
     53      0.9996        0.0001        0.0013     +  119.8323
     54      0.9996        0.0001        0.0013        119.8553
     55      0.9996        0.0001        0.0013     +  119.9463
     56      0.9996        0.0001        0.0013        120.2165
     57      0.9995        0.0001        0.0013        119.6972
     58      0.9995        0.0001        0.0013        119.7742
     59      0.9994        0.0001        0.0015        119.6781
     60      0.9996        0.0001        0.0012     +  119.6171
     61      0.9996        0.0001        0.0012     +  119.5510
     62      0.9996        0.0001        0.0012     +  119.7072
     63      0.9996        0.0001        0.0012        119.8723
     64      0.9995        0.0001        0.0012        119.6361
     65      0.9996        0.0001        0.0012     +  119.7462
     66      0.9996        0.0001        0.0012        119.5981
     67      0.9996        0.0001        0.0012        119.7702
     68      0.9996        0.0001        0.0012     +  119.6061
     69      0.9995        0.0001        0.0012        119.5651
     70      0.9996        0.0001        0.0012     +  120.4757
     71      0.9996        0.0001        0.0012        119.8433
     72      0.9996        0.0001        0.0012        119.9383
     73      0.9996        0.0001        0.0012     +  120.2596
     74      0.9995        0.0001        0.0013        120.3366
     75      0.9996        0.0001        0.0013        120.1635
     76      0.9995        0.0001        0.0012        120.2556
     77      0.9996        0.0001        0.0012        120.2416
Re-initializing optimizer because the following parameters were re-set: lr.
     78      0.9995        0.0001        0.0013        120.1465
     79      0.9996        0.0000        0.0012        120.0634
     80      0.9996        0.0001        0.0012     +  119.9303
     81      0.9996        0.0001        0.0012     +  119.9483
     82      0.9996        0.0001        0.0012     +  119.9383
     83      0.9996        0.0001        0.0012        120.0894
     84      0.9996        0.0001        0.0012        120.0174
     85      0.9996        0.0001        0.0012        120.1255
     86      0.9996        0.0000        0.0012        120.3637
Re-initializing optimizer because the following parameters were re-set: lr.
     87      0.9996        0.0000        0.0012        120.2195
     88      0.9996        0.0001        0.0012        119.8493
     89      0.9996        0.0000        0.0012     +  120.2125
     90      0.9996        0.0001        0.0012        119.8663
     91      0.9996        0.0000        0.0012        119.9193
     92      0.9996        0.0000        0.0012        119.7012
     93      0.9996        0.0001        0.0012        119.7872
Re-initializing optimizer because the following parameters were re-set: lr.
     94      0.9996        0.0001        0.0012        119.7572
     95      0.9996        0.0000        0.0012        119.7812
     96      0.9996        0.0001        0.0012        119.6621
     97      0.9996        0.0000        0.0012        119.6071
     98      0.9996        0.0001        0.0012        119.8252
     99      0.9996        0.0001        0.0012        119.6471
    100      0.9996        0.0001        0.0012        119.7252
    101      0.9996        0.0001        0.0012        119.8403
    102      0.9995        0.0001        0.0012        119.6671
    103      0.9996        0.0001        0.0012        119.7222
    104      0.9996        0.0001        0.0012        119.8222
    105      0.9996        0.0000        0.0012        120.0084
    106      0.9996        0.0000        0.0012        119.8533
    107      0.9996        0.0000        0.0012        119.6511
    108      0.9996        0.0001        0.0012        119.7612
Stopping since valid_loss has not improved in the last 20 epochs.
Best Model State Restored
Best Confusion Matrix:
 [[100   0]
 [  0 100]]
