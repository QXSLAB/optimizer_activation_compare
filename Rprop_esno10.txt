Namespace(activation='relu', arch='resnet18', cuda='3', optimizer='Rprop', root='D:/qpsk_awgn_sps8_esno10.dat')
ResNet(
  (conv1): Conv1d(2, 64, kernel_size=(7,), stride=(2,), padding=(3,), bias=False)
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace)
  (maxpool): MaxPool1d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(128, 256, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv1d(256, 512, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv1d(256, 512, kernel_size=(1,), stride=(2,), bias=False)
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace)
      (conv2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)
      (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (fc): Linear(in_features=512, out_features=2, bias=True)
)
loading data
  epoch    accuracy    train_loss    valid_loss    cp       dur
-------  ----------  ------------  ------------  ----  --------
      1      0.8187        0.4226        0.3999     +  874.4213
      2      0.8203        0.3903        0.3963     +  878.7705
      3      0.8220        0.3869        0.3934     +  883.4310
      4      0.8238        0.3839        0.3906     +  884.9731
      5      0.8250        0.3811        0.3882     +  871.8554
      6      0.8260        0.3787        0.3862     +  867.3130
      7      0.8283        0.3762        0.3834     +  869.9419
      8      0.8294        0.3736        0.3811     +  887.7112
      9      0.8298        0.3716        0.3793     +  887.2829
     10      0.8319        0.3689        0.3768     +  888.0554
     11      0.8334        0.3670        0.3747     +  887.5411
     12      0.8342        0.3648        0.3726     +  888.3196
     13      0.8348        0.3630        0.3709     +  887.7412
     14      0.8358        0.3606        0.3689     +  888.9991
     15      0.8365        0.3587        0.3672     +  887.3019
     16      0.8381        0.3566        0.3649     +  909.6332
     17      0.8389        0.3547        0.3630     +  888.3026
     18      0.8397        0.3529        0.3613     +  889.1322
     19      0.8405        0.3514        0.3595     +  887.4730
     20      0.8410        0.3494        0.3578     +  888.4928
     21      0.8418        0.3475        0.3562     +  887.5080
     22      0.8426        0.3457        0.3545     +  887.9193
     23      0.8436        0.3438        0.3528     +  887.8243
     24      0.8441        0.3425        0.3513     +  887.9233
     25      0.8456        0.3406        0.3498     +  888.1475
     26      0.8462        0.3393        0.3483     +  888.1675
     27      0.8469        0.3375        0.3471     +  888.0735
     28      0.8470        0.3360        0.3455     +  887.3449
     29      0.8476        0.3348        0.3440     +  887.3679
     30      0.8485        0.3331        0.3428     +  887.8433
     31      0.8488        0.3320        0.3420     +  887.1297
     32      0.8503        0.3299        0.3401     +  888.3196
     33      0.8509        0.3290        0.3388     +  887.8913
     34      0.8517        0.3277        0.3378     +  887.2498
     35      0.8526        0.3262        0.3364     +  887.3699
     36      0.8527        0.3248        0.3353     +  887.0017
     37      0.8537        0.3233        0.3342     +  889.2393
     38      0.8544        0.3223        0.3330     +  887.2348
     39      0.8550        0.3213        0.3319     +  887.2648
     40      0.8550        0.3203        0.3313     +  886.0980
     41      0.8556        0.3189        0.3300     +  886.3452
     42      0.8564        0.3173        0.3289     +  887.1698
     43      0.8567        0.3168        0.3279     +  886.4232
     44      0.8571        0.3154        0.3269     +  887.5381
     45      0.8576        0.3146        0.3260     +  887.1067
     46      0.8583        0.3131        0.3250     +  886.5993
     47      0.8581        0.3118        0.3243     +  888.8550
     48      0.8588        0.3112        0.3234     +  887.4390
     49      0.8593        0.3101        0.3225     +  888.0764
     50      0.8593        0.3090        0.3221     +  886.9856
     51      0.8600        0.3085        0.3208     +  887.7782
     52      0.8602        0.3073        0.3200     +  887.7122
     53      0.8607        0.3068        0.3194     +  886.8435
     54      0.8611        0.3052        0.3185     +  887.4990
     55      0.8610        0.3048        0.3177     +  887.3709
     56      0.8617        0.3032        0.3170     +  888.1375
     57      0.8624        0.3023        0.3162     +  886.6984
     58      0.8628        0.3016        0.3156     +  886.4182
     59      0.8630        0.3008        0.3148     +  882.7295
     60      0.8629        0.3001        0.3144     +  934.7242
     61      0.8635        0.2993        0.3136     +  930.1628
     62      0.8638        0.2983        0.3128     +  933.7014
     63      0.8646        0.2975        0.3122     +  931.7760
     64      0.8647        0.2967        0.3116     +  931.9561
     65      0.8652        0.2958        0.3109     +  936.8428
     66      0.8656        0.2953        0.3102     +  937.1700
     67      0.8657        0.2945        0.3097     +  939.0074
     68      0.8661        0.2932        0.3090     +  939.1165
     69      0.8659        0.2925        0.3087     +  936.7487
     70      0.8666        0.2926        0.3080     +  926.2459
     71      0.8667        0.2914        0.3074     +  924.2684
     72      0.8671        0.2907        0.3069     +  956.9578
     73      0.8674        0.2899        0.3061     +  960.6015
     74      0.8679        0.2891        0.3056     +  958.7521
     75      0.8680        0.2887        0.3051     +  961.4331
     76      0.8686        0.2876        0.3045     +  934.3009
     77      0.8686        0.2872        0.3040     +  923.3717
     78      0.8695        0.2864        0.3035     +  921.1251
     79      0.8696        0.2856        0.3029     +  938.2128
     80      0.8695        0.2851        0.3024     +  950.3859
     81      0.8698        0.2842        0.3018     +  951.0383
     82      0.8700        0.2837        0.3014     +  950.7631
     83      0.8707        0.2828        0.3010     +  929.3312
     84      0.8706        0.2824        0.3006     +  917.0460
     85      0.8702        0.2818        0.3005     +  918.5812
     86      0.8703        0.2809        0.3002     +  947.0153
     87      0.8714        0.2802        0.2993     +  949.2980
     88      0.8722        0.2799        0.2988     +  950.0646
     89      0.8721        0.2790        0.2982     +  932.7107
     90      0.8718        0.2787        0.2982     +  921.2442
     91      0.8726        0.2776        0.2974     +  921.3853
     92      0.8728        0.2774        0.2970     +  923.9922
     93      0.8723        0.2766        0.2967     +  919.8671
     94      0.8730        0.2761        0.2962     +  929.9737
     95      0.8727        0.2753        0.2958     +  930.9634
     96      0.8733        0.2752        0.2953     +  927.0205
     97      0.8738        0.2739        0.2948     +  928.1413
     98      0.8742        0.2740        0.2949        924.8639
     99      0.8741        0.2726        0.2941     +  930.4930
    100      0.8743        0.2726        0.2937     +  929.7565
    101      0.8735        0.2721        0.2937     +  919.1126
    102      0.8749        0.2717        0.2931     +  924.3785
    103      0.8752        0.2707        0.2925     +  920.1013
    104      0.8753        0.2702        0.2923     +  928.3855
    105      0.8750        0.2696        0.2919     +  924.7167
    106      0.8757        0.2692        0.2917     +  927.1465
    107      0.8759        0.2685        0.2913     +  930.9224
    108      0.8751        0.2681        0.2909     +  948.1882
    109      0.8750        0.2674        0.2907     +  947.1004
    110      0.8756        0.2666        0.2902     +  939.9601
    111      0.8757        0.2662        0.2897     +  934.2068
    112      0.8766        0.2657        0.2895     +  926.5131
    113      0.8758        0.2654        0.2895     +  916.4776
    114      0.8759        0.2647        0.2891     +  924.9019
    115      0.8761        0.2644        0.2886     +  924.8598
    116      0.8762        0.2639        0.2884     +  925.2571
    117      0.8769        0.2635        0.2883     +  938.5120
    118      0.8768        0.2625        0.2879     +  937.0359
    119      0.8770        0.2623        0.2877     +  926.4971
    120      0.8773        0.2614        0.2876     +  937.0009
    121      0.8771        0.2616        0.2870     +  935.0494
    122      0.8774        0.2605        0.2866     +  938.6381
    123      0.8774        0.2605        0.2866        936.3754
    124      0.8774        0.2600        0.2865     +  938.7742
    125      0.8773        0.2591        0.2859     +  920.8569
    126      0.8780        0.2592        0.2858     +  911.8071
    127      0.8784        0.2578        0.2852     +  914.8584
    128      0.8785        0.2584        0.2851     +  918.7263
    129      0.8780        0.2575        0.2848     +  916.3195
    130      0.8780        0.2571        0.2849        918.1148
    131      0.8785        0.2566        0.2843     +  915.6800
    132      0.8789        0.2560        0.2840     +  916.5477
    133      0.8789        0.2556        0.2839     +  916.3105
    134      0.8790        0.2551        0.2837     +  913.0901
    135      0.8790        0.2545        0.2834     +  899.7181
    136      0.8792        0.2544        0.2831     +  891.1427
    137      0.8792        0.2539        0.2830     +  894.8765
    138      0.8787        0.2533        0.2837        892.4657
    139      0.8794        0.2526        0.2827     +  891.9623
    140      0.8794        0.2530        0.2826     +  891.7682
    141      0.8801        0.2518        0.2821     +  889.9018
    142      0.8801        0.2516        0.2818     +  902.8234
    143      0.8801        0.2512        0.2817     +  905.4474
    144      0.8802        0.2505        0.2820        902.7404
    145      0.8802        0.2506        0.2816     +  919.8942
    146      0.8804        0.2500        0.2814     +  924.2854
    147      0.8804        0.2493        0.2809     +  920.3435
    148      0.8806        0.2491        0.2810        919.0545
    149      0.8799        0.2484        0.2812        919.6320
    150      0.8809        0.2483        0.2803     +  918.9895
    151      0.8806        0.2480        0.2801     +  919.4258
    152      0.8808        0.2475        0.2800     +  920.4215
    153      0.8811        0.2472        0.2802        921.9837
    154      0.8810        0.2467        0.2797     +  902.8214
    155      0.8810        0.2461        0.2797        887.3039
    156      0.8811        0.2452        0.2795     +  886.5543
    157      0.8814        0.2452        0.2791     +  886.9976
    158      0.8815        0.2446        0.2792        887.3719
    159      0.8814        0.2446        0.2790     +  887.3919
    160      0.8817        0.2435        0.2792        887.8493
    161      0.8814        0.2435        0.2786     +  888.2146
    162      0.8817        0.2431        0.2787        887.9373
    163      0.8818        0.2425        0.2783     +  887.3059
    164      0.8820        0.2422        0.2782     +  887.4650
    165      0.8824        0.2416        0.2785        887.9544
    166      0.8821        0.2415        0.2780     +  887.5981
